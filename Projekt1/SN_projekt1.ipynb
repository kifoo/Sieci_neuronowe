{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kifoo/Sieci_neuronowe/blob/main/SN_projekt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu-gyo2YOpm0"
      },
      "source": [
        "# Kod implementacji płystkiej sieci neuronowej"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ_eVqiHk1R0"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo\n",
        "# !pip import pandas\n",
        "# !pip install -U scikit-learn\n",
        "# !pip install keras\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ynTU31PgGp"
      },
      "source": [
        "Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KBYhrl27kiaB"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "DLL load failed while importing _pywrap_tf2: Procedura inicjowania biblioteki dołączanej dynamicznie (DLL) nie powiodła się.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\magda\\OneDrive\\Pulpit\\Semestr5\\Sieci_neuronowe\\Projekt1\\SN_projekt1.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/Projekt1/SN_projekt1.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder, StandardScaler\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/Projekt1/SN_projekt1.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/Projekt1/SN_projekt1.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/Projekt1/SN_projekt1.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m resolver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mcluster_resolver\u001b[39m.\u001b[39mTPUClusterResolver(tpu\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://colab.research.google.com/drive/1jT81C7kyoC3rkRtzP4XB8U0yMsHRMMWs?usp=sharing\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/Projekt1/SN_projekt1.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental_connect_to_cluster(resolver)\n",
            "File \u001b[1;32mc:\\Users\\magda\\OneDrive\\Pulpit\\Semestr5\\Sieci_neuronowe\\venv\\Lib\\site-packages\\tensorflow\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m _os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mTF2_BEHAVIOR\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2 \u001b[39mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[39m.\u001b[39menable()\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n",
            "File \u001b[1;32mc:\\Users\\magda\\OneDrive\\Pulpit\\Semestr5\\Sieci_neuronowe\\venv\\Lib\\site-packages\\tensorflow\\python\\tf2.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Tools to help with the TensorFlow 2.0 transition.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mThis module is meant for TensorFlow internal implementation, not for users of\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mthe TensorFlow library. For that see tf.compat instead.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_tf2\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menable\u001b[39m():\n\u001b[0;32m     26\u001b[0m   \u001b[39m# Enables v2 behaviors.\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: Procedura inicjowania biblioteki dołączanej dynamicznie (DLL) nie powiodła się."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from ucimlrepo import fetch_ucirepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4FARe1lPw1J"
      },
      "source": [
        "Function fetching dataset from https://archive.ics.uci.edu/dataset/602/dry+bean+dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKhQxVvKkiRJ"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def data_fetch():\n",
        "    # fetch dataset\n",
        "    dry_bean_dataset = fetch_ucirepo(id=602)\n",
        "\n",
        "    # data (as pandas dataframes)\n",
        "    X = dry_bean_dataset.data.features\n",
        "    Y = dry_bean_dataset.data.targets\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    Y = label_encoder.fit_transform(Y)\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i08Ea3I-QBYF"
      },
      "source": [
        "Function defining the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_KZ0V0ipHZP"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Define the neural network model\n",
        "def create_model(input_shape, output_shape, nuerons_output, hidden_layers):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nuerons_output, activation='relu', input_shape=(input_shape,)))\n",
        "    for i in range(hidden_layers):\n",
        "        model.add(Dense(nuerons_output, activation='relu'))\n",
        "    model.add(Dense(output_shape, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqgwyc47Qa_b"
      },
      "source": [
        "Creating the neural network model, training it and testing it accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX7LD1PFnBDF"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def neural_network(opt_function, loss_function, nuerons_output, hidden_layers, epochs):\n",
        "    # This function return the dataframe\n",
        "    X, Y = data_fetch()\n",
        "\n",
        "    # Split the dataset into a training set and a testing set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
        "\n",
        "    # Normalize the inputs\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # print(\"========== Train ==========\")\n",
        "    # print(y_train.shape[0])\n",
        "    # print(\"========== Test ==========\")\n",
        "    # print(X_train.shape[1])\n",
        "\n",
        "    # Create the neural network model\n",
        "    model = create_model(input_shape=X_train.shape[1], output_shape=7, nuerons_output=nuerons_output, hidden_layers=hidden_layers)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=opt_function, loss=loss_function, metrics='accuracy')\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs)\n",
        "\n",
        "    # Test the model\n",
        "    _, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tYXsFZ9HYUd"
      },
      "source": [
        "Loss function exapmles:\n",
        "\n",
        "**'mean_squared_error' or 'mse':** Measures the average squared difference between\n",
        "the true and predicted labels. It is useful for regression problems.\n",
        "\n",
        "**'binary_crossentropy':** Applies a binary cross-entropy between the true and predicted labels. It is useful for binary classification problems.\n",
        "\n",
        "**'categorical_crossentropy':** Applies a categorical cross-entropy between the true and predicted labels. It is useful for multi-class classification problems.\n",
        "\n",
        "**'hinge':** Defines a hinge loss function that measures the difference between the predicted labels and true labels.\n",
        "\n",
        "**'logcosh':** Defines a logarithm of the hyperbolic cosine loss function.\n",
        "\n",
        "**'sparse_categorical_crossentropy':** Similar to 'categorical_crossentropy', but it can accept labels with integer values instead of one-hot encoded vectors.\n",
        "\n",
        "**'kl_divergence':** Calculates the Kullback-Leibler divergence, which quantifies the difference between two probability distributions.\n",
        "\n",
        "**'poisson':** Calculates the Poisson loss between the true and predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lN9cWNDKOna",
        "outputId": "4cf18327-10c5-4f93-9d58-44ab0343e945"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/magda/OneDrive/Pulpit/Semestr5/Sieci_neuronowe/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def write_data(neuron_outputs,hidden_layers,epochs,acc):\n",
        "    csv_file_path =\"content/data.csv\"\n",
        "\n",
        "    with open(csv_file_path, 'a', newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([neuron_outputs, hidden_layers, epochs, acc])\n",
        "        print(\"ok\\n\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    loss = ['mean_squared_error', 'binary_crossentropy', 'categorical_crossentropy', 'hinge', 'logcosh', 'sparse_categorical_crossentropy', 'kl_divergence']\n",
        "    neuron_outputs = [2, 4, 8, 12, 16]\n",
        "    hidden_layers = [0, 1, 2]  # góra-cy, ale liczymy od 0 jak prawdziwi informatycy XDD\n",
        "    epochs = [5, 10, 25, 50]\n",
        "\n",
        "    # Ej bo tu wychodzi jakieś 60 danych XDD Dokładnie to 63, nie wiem czemu XDD\n",
        "    min_acc = 1\n",
        "    min_data = np.array([[0], [0], [0], [0.0]])\n",
        "    max_acc = 0\n",
        "    max_data = np.array([[0], [0], [0], [0.0]])\n",
        "    write_data(\"Neuron Output\",\"Hidden Layers\",\"Epochs\",\"Accuracy\")\n",
        "\n",
        "    for a in range(len(neuron_outputs)):\n",
        "          for b in range(len(hidden_layers)):\n",
        "              for c in range(len(epochs)):\n",
        "                  acc = neural_network(opt_function='Adam', loss_function=loss[5], nuerons_output=neuron_outputs[a], hidden_layers=hidden_layers[b], epochs=epochs[c])\n",
        "                  if( acc > max_acc):\n",
        "                      max_acc = acc\n",
        "                      max_data = [[neuron_outputs[a]], [hidden_layers[b]], [epochs[c]], [acc]]\n",
        "                  if( acc < min_acc):\n",
        "                      min_acc = acc\n",
        "                      min_data = [[neuron_outputs[a]], [hidden_layers[b]], [epochs[c]], [acc]]\n",
        "                  write_data(neuron_outputs[a],hidden_layers[b],epochs[c],acc)\n",
        "\n",
        "    print(f\"Min accuracy: {min_data} \\nMax accuracy: {max_data}\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV8Aj7AyuC3d"
      },
      "source": [
        "Min accuracy: [[2], [2], [50], [0.2634440064430237]]\n",
        "\n",
        "Max accuracy: [[16], [1], [50], [0.9315310120582581]]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
